# Configuração ChromaDB (servidor remoto)
CHROMA_HOST=localhost
CHROMA_PORT=8001

# Configuração Ollama (LLM Local) - RECOMENDADO
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Configuração LLM
# Opções de provedor: ollama (ou llama) para local, openai, anthropic
LLM_PROVIDER=ollama
# Modelos Ollama/Llama: llama3.1:8b, llama3, llama2, mistral, phi, codellama, etc.
LLM_MODEL=llama3.1:8b

# Chaves de API LLM (apenas se usar openai ou anthropic)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Configuração de Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Configuração Chainlit
CHAINLIT_HOST=0.0.0.0
CHAINLIT_PORT=8000

# Modo de Debug
DEBUG=False
