# ğŸ“š Agente Q&A com PDF usando RAG

Sistema de perguntas e respostas baseado em documentos PDF usando RAG (Retrieval-Augmented Generation), Chainlit, LangGraph e ChromaDB.

## ğŸ¯ VisÃ£o Geral

Este projeto demonstra como construir um agente inteligente de Q&A que:

- **Processa documentos PDF** e extrai texto com preservaÃ§Ã£o de metadados
- **Indexa conteÃºdo** em um banco de dados vetorial (ChromaDB)
- **Responde perguntas** baseado no conteÃºdo dos documentos
- **Cita fontes** com nÃºmero de pÃ¡gina e relevÃ¢ncia
- **Interface interativa** via Chainlit com upload de arquivos
- **Arquitetura modular** usando LangGraph para workflow do agente
- **Totalmente dockerizado** com ChromaDB em container separado

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Docker Compose                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ChromaDB    â”‚  â”‚   Ollama     â”‚  â”‚   RAG App       â”‚   â”‚
â”‚  â”‚  (Vector DB) â”‚  â”‚   (LLM)      â”‚  â”‚   (Chainlit)    â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚                 â”‚   â”‚
â”‚  â”‚  Port: 8001  â”‚â—„â”€â”¤  Port: 11434 â”‚â—„â”€â”¤  - PDF Parser  â”‚   â”‚
â”‚  â”‚  (external)  â”‚  â”‚  (external)  â”‚  â”‚  - LangGraph    â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  - Embeddings   â”‚   â”‚
â”‚  â”‚  Volume:     â”‚  â”‚  Volume:     â”‚  â”‚  - Agent        â”‚   â”‚
â”‚  â”‚  chromadb    â”‚  â”‚  ollama      â”‚  â”‚                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Port: 8000     â”‚   â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

1. **Ollama** - LLM local gratuito (llama3, mistral, etc.) - **RECOMENDADO**
2. **ChromaDB** - Banco de dados vetorial para armazenamento e busca semÃ¢ntica
3. **LangGraph** - OrquestraÃ§Ã£o do workflow do agente (anÃ¡lise â†’ recuperaÃ§Ã£o â†’ geraÃ§Ã£o)
4. **Chainlit** - Interface web interativa para chat e upload de PDFs
5. **LangChain** - AbstraÃ§Ãµes para LLMs e embeddings
6. **PyMuPDF** - ExtraÃ§Ã£o de texto de PDFs

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- **8 GB de RAM** (para rodar Ollama com llama3)
- **Opcional**: Chave de API da OpenAI ou Anthropic (se nÃ£o quiser usar Ollama)

### InstalaÃ§Ã£o e ExecuÃ§Ã£o

1. **Clone o repositÃ³rio**:
```bash
git clone <seu-repositorio>
cd project
```

2. **Configure as variÃ¡veis de ambiente**:
```bash
cp .env.example .env
# O arquivo jÃ¡ vem configurado para usar Ollama (gratuito)
# NÃ£o precisa de chaves de API!
```

3. **Inicie a aplicaÃ§Ã£o**:
```bash
docker-compose up -d
```

4. **Baixe o modelo Ollama** (primeira vez apenas):
```bash
# Aguarde o Ollama iniciar (1-2 minutos)
docker-compose logs -f ollama

# Baixar llama3.1:8b (~4.7 GB) - modelo padrÃ£o
docker exec -it ollama-server ollama pull llama3.1:8b

# Verificar download
docker exec -it ollama-server ollama list
```

5. **Acesse a interface**:
- **AplicaÃ§Ã£o Chainlit**: http://localhost:8000
- **ChromaDB Admin** (opcional): http://localhost:8001
- **Ollama API** (opcional): http://localhost:11434

6. **Visualize os logs**:
```bash
# Todos os serviÃ§os
docker-compose logs -f

# Apenas Ollama
docker-compose logs -f ollama

# Apenas aplicaÃ§Ã£o
docker-compose logs -f rag-app
```

## ğŸ“– Como Usar

### 1. Fazer Upload de PDFs

1. Acesse http://localhost:8000
2. Clique no Ã­cone de anexo (ğŸ“)
3. Selecione um ou mais arquivos PDF
4. Aguarde o processamento e indexaÃ§Ã£o

### 2. Fazer Perguntas

Digite suas perguntas no chat. Exemplos:

```
"Quais sÃ£o os principais tÃ³picos discutidos no documento?"
"Explique o conceito de X mencionado no artigo"
"Quais sÃ£o as conclusÃµes apresentadas?"
```

### 3. Ver Fontes

As respostas incluem automaticamente:
- Nome do documento fonte
- NÃºmero da pÃ¡gina
- Score de relevÃ¢ncia

## ğŸ› ï¸ ConfiguraÃ§Ã£o

### ğŸ¦™ Usando Ollama (PadrÃ£o - Gratuito)

O projeto vem configurado para usar **Ollama** por padrÃ£o, um LLM local e gratuito!

**Modelos suportados**:
- `llama3.1:8b` - **PadrÃ£o** - Recomendado (4.7 GB)
- `llama3` - VersÃ£o anterior (4.7 GB)
- `mistral` - Mais rÃ¡pido (4.1 GB)
- `phi` - Mais leve (1.6 GB)
- `codellama` - Para cÃ³digo (3.8 GB)

**Como trocar de modelo**:
```bash
# Baixar novo modelo
docker exec -it ollama-server ollama pull mistral

# Atualizar .env
LLM_MODEL=mistral

# Reiniciar aplicaÃ§Ã£o
docker-compose restart rag-app
```

ğŸ“– **Guia completo**: Veja [OLLAMA.md](OLLAMA.md) para detalhes, otimizaÃ§Ãµes e troubleshooting.

### ğŸ”‘ Usando APIs Externas (Opcional)

Se preferir usar OpenAI ou Anthropic:

```env
# ConfiguraÃ§Ã£o LLM
LLM_PROVIDER=openai  # ou anthropic
LLM_MODEL=gpt-4-turbo-preview

# Adicionar chave de API
OPENAI_API_KEY=sua_chave_aqui
```

### VariÃ¡veis de Ambiente (.env)

```env
# Ollama (LLM Local) - PADRÃƒO
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b

# Opcional: APIs externas
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# ChromaDB
CHROMA_HOST=localhost
CHROMA_PORT=8001

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

### ConfiguraÃ§Ã£o YAML (config.yaml)

```yaml
chunking:
  tamanho_chunk: 1000
  sobreposicao_chunk: 200
  estrategia: "fixed"

recuperacao:
  top_k: 5
  limiar_similaridade: 0.7
  usar_reranking: false

llm:
  temperatura: 0.7
  max_tokens: 2000
  streaming: true

upload:
  max_tamanho_mb: 50
  formatos_permitidos:
    - "pdf"
```

## ğŸ³ Comandos Docker

### Desenvolvimento

```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Ver logs em tempo real
docker-compose logs -f

# Ver logs apenas da aplicaÃ§Ã£o
docker-compose logs -f rag-app

# Ver logs apenas do ChromaDB
docker-compose logs -f chromadb

# Parar serviÃ§os
docker-compose down

# Reconstruir a aplicaÃ§Ã£o
docker-compose build rag-app

# Reiniciar apenas a aplicaÃ§Ã£o
docker-compose restart rag-app
```

### Limpeza

```bash
# Parar e remover containers
docker-compose down

# Parar e remover containers + volumes (CUIDADO: apaga dados)
docker-compose down -v

# Remover imagens nÃ£o utilizadas
docker image prune -a
```

## ğŸ“ Estrutura do Projeto

```
project/
â”œâ”€â”€ src/llm_rag/
â”‚   â”œâ”€â”€ config.py              # Gerenciamento de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ embeddings.py          # Modelos de embedding
â”‚   â”œâ”€â”€ pdf_processor.py       # Processamento de PDF
â”‚   â”œâ”€â”€ vector_store.py        # Interface ChromaDB
â”‚   â”œâ”€â”€ agent.py               # Agente principal
â”‚   â”œâ”€â”€ graph/                 # LangGraph workflow
â”‚   â”‚   â”œâ”€â”€ state.py          # Estado do grafo
â”‚   â”‚   â”œâ”€â”€ nodes.py          # NÃ³s do grafo
â”‚   â”‚   â””â”€â”€ workflow.py       # DefiniÃ§Ã£o do workflow
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app.py            # Interface Chainlit
â”œâ”€â”€ tests/                     # Testes
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/              # PDFs enviados
â”‚   â””â”€â”€ chroma/               # Dados ChromaDB (em volume)
â”œâ”€â”€ Dockerfile                # Container da aplicaÃ§Ã£o
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o
â”œâ”€â”€ config.yaml              # ConfiguraÃ§Ãµes
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente
â””â”€â”€ pyproject.toml           # DependÃªncias
```

## ğŸ§ª Testes

### Executar Testes Localmente

```bash
# Instalar dependÃªncias
poetry install --with dev

# Executar todos os testes
poetry run pytest

# Executar com cobertura
poetry run pytest --cov=llm_rag

# Executar testes especÃ­ficos
poetry run pytest tests/test_config.py
```

### Executar Testes no Docker

```bash
docker-compose exec rag-app pytest
```

## ğŸ”§ Desenvolvimento Local (Sem Docker)

### 1. Instalar DependÃªncias

```bash
# Instalar Poetry
curl -sSL https://install.python-poetry.org | python3 -

# Instalar dependÃªncias do projeto
poetry install --with dev

# Ativar ambiente virtual
poetry shell
```

### 2. Configurar Ambiente

```bash
cp .env.example .env
# Editar .env com suas chaves de API
```

### 3. Iniciar ChromaDB Localmente

```bash
# OpÃ§Ã£o 1: Usar Docker apenas para ChromaDB
docker run -d -p 8001:8000 \
  -v chromadb-data:/chroma/chroma \
  -e IS_PERSISTENT=TRUE \
  chromadb/chroma:latest

# OpÃ§Ã£o 2: Instalar ChromaDB localmente
pip install chromadb
chroma run --path ./data/chroma --port 8001
```

### 4. Executar AplicaÃ§Ã£o

```bash
chainlit run src/llm_rag/ui/app.py --host 0.0.0.0 --port 8000
```

## ğŸ“Š Workflow do Agente (LangGraph)

```mermaid
graph TD
    A[InÃ­cio] --> B[Analisar Consulta]
    B -->|Precisa RecuperaÃ§Ã£o| C[Recuperar Documentos]
    B -->|NÃ£o Precisa| F[Gerar Resposta]
    C --> D[Formatar Contexto]
    D --> E[Gerar Resposta com Contexto]
    E --> G[Fim]
    F --> G
```

### NÃ³s do Grafo

1. **Analisar Consulta** - Determina se precisa buscar documentos
2. **Recuperar Documentos** - Busca chunks relevantes no ChromaDB
3. **Formatar Contexto** - Organiza documentos recuperados
4. **Gerar Resposta** - Usa LLM para gerar resposta fundamentada

## ğŸ”’ SeguranÃ§a

- Nunca commite o arquivo `.env` com chaves de API
- Use variÃ¡veis de ambiente para informaÃ§Ãµes sensÃ­veis
- Limite o tamanho de upload de PDFs (padrÃ£o: 50MB)
- Valide formatos de arquivo antes do processamento

## ğŸ› Troubleshooting

### Erro: "NÃ£o foi possÃ­vel conectar ao ChromaDB"

```bash
# Verifique se o ChromaDB estÃ¡ rodando
docker-compose ps

# Verifique os logs do ChromaDB
docker-compose logs chromadb

# Reinicie o ChromaDB
docker-compose restart chromadb
```

### Erro: "OPENAI_API_KEY nÃ£o configurada"

```bash
# Verifique se o .env existe e estÃ¡ preenchido
cat .env

# Recrie o container com novas variÃ¡veis
docker-compose down
docker-compose up -d
```

### Erro: "Out of Memory"

```bash
# Aumente recursos do Docker Desktop
# Ou reduza tamanho dos chunks em config.yaml

chunking:
  tamanho_chunk: 500  # Reduzir de 1000 para 500
```

### PDFs nÃ£o sÃ£o processados

```bash
# Verifique permissÃµes do diretÃ³rio de uploads
ls -la data/uploads/

# Verifique logs da aplicaÃ§Ã£o
docker-compose logs rag-app

# Verifique formato e integridade do PDF
file seu_arquivo.pdf
```

## ğŸ“ˆ Melhorias Futuras

- [ ] Suporte para mais formatos (DOCX, TXT, Markdown)
- [ ] Re-ranking de resultados com modelos especializados
- [ ] Conversational memory com histÃ³rico persistente
- [ ] Suporte multi-idioma
- [ ] AutenticaÃ§Ã£o de usuÃ¡rios
- [ ] API REST para integraÃ§Ã£o
- [ ] MÃ©tricas e observabilidade (Prometheus + Grafana)
- [ ] Deployment em cloud (AWS, GCP, Azure)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¥ Autores

- **Flavio De Oliveira Vieira** - flovieira@rd.com.br

## ğŸ™ Agradecimentos

- [Chainlit](https://chainlit.io/) - Framework para interfaces de chat
- [LangGraph](https://github.com/langchain-ai/langgraph) - OrquestraÃ§Ã£o de agentes
- [ChromaDB](https://www.trychroma.com/) - Banco de dados vetorial
- [LangChain](https://www.langchain.com/) - Framework LLM
- [PyMuPDF](https://pymupdf.readthedocs.io/) - ManipulaÃ§Ã£o de PDFs

## ğŸ“ Suporte

Para questÃµes e suporte:
- Abra uma issue no GitHub
- Entre em contato: flovieira@rd.com.br

---

**Desenvolvido com â¤ï¸ usando Python, LangChain e Docker**
