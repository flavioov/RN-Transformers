# ü¶ô Guia Ollama - LLM Local

Este guia explica como usar o Ollama como modelo de linguagem local no projeto.

## üéØ Vantagens do Ollama

- ‚úÖ **100% Gratuito** - Sem custos de API
- ‚úÖ **Privacidade Total** - Dados n√£o saem do seu ambiente
- ‚úÖ **Sem Limites** - Use quanto quiser sem cotas
- ‚úÖ **Offline** - Funciona sem conex√£o com internet
- ‚úÖ **R√°pido** - Baixa lat√™ncia em hardware adequado

## üìã Requisitos

### Hardware M√≠nimo Recomendado

- **CPU**: 4 cores
- **RAM**: 8 GB (16 GB recomendado)
- **Disco**: 10 GB livres
- **GPU**: Opcional (NVIDIA recomendada para melhor performance)

### Modelos Dispon√≠veis

| Modelo | Tamanho | RAM Necess√°ria | Velocidade | Qualidade |
|--------|---------|----------------|------------|-----------|
| llama3 | ~4.7 GB | 8 GB | R√°pido | Excelente |
| llama2 | ~3.8 GB | 8 GB | R√°pido | Boa |
| mistral | ~4.1 GB | 8 GB | Muito R√°pido | Excelente |
| phi | ~1.6 GB | 4 GB | Muito R√°pido | Boa |
| codellama | ~3.8 GB | 8 GB | R√°pido | Boa (c√≥digo) |

## üöÄ Configura√ß√£o R√°pida

### 1. Usando Docker Compose (Recomendado)

O projeto j√° vem configurado com Ollama! Basta iniciar:

```bash
# Copiar configura√ß√£o de exemplo
cp .env.example .env

# Iniciar todos os servi√ßos (inclui Ollama)
docker-compose up -d

# Aguardar inicializa√ß√£o (1-2 minutos)
docker-compose logs -f ollama
```

### 2. Baixar Modelo Llama3

```bash
# Entrar no container Ollama
docker exec -it ollama-server ollama pull llama3

# Verificar modelos instalados
docker exec -it ollama-server ollama list
```

### 3. Testar Ollama

```bash
# Testar modelo diretamente
docker exec -it ollama-server ollama run llama3 "Ol√°, como voc√™ est√°?"

# Verificar API
curl http://localhost:11434/api/tags
```

## üîß Configura√ß√£o Manual (Sem Docker)

### Instalar Ollama Localmente

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### macOS
```bash
brew install ollama
```

#### Windows
Baixe o instalador em: https://ollama.com/download/windows

### Iniciar Servidor Ollama

```bash
# Iniciar servidor (porta padr√£o 11434)
ollama serve

# Em outro terminal, baixar modelo
ollama pull llama3

# Testar
ollama run llama3 "Ol√°!"
```

### Configurar Aplica√ß√£o

```bash
# Editar .env
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
LLM_PROVIDER=ollama
LLM_MODEL=llama3
```

## üé® Modelos Recomendados por Caso de Uso

### Para Q&A com PDFs (Nosso Caso)
```bash
docker exec -it ollama-server ollama pull llama3
```
- **Modelo**: llama3
- **Motivo**: Excelente compreens√£o de contexto e portugu√™s

### Para C√≥digo
```bash
docker exec -it ollama-server ollama pull codellama
```
- **Modelo**: codellama
- **Motivo**: Especializado em c√≥digo

### Para Performance M√°xima
```bash
docker exec -it ollama-server ollama pull mistral
```
- **Modelo**: mistral
- **Motivo**: Muito r√°pido e eficiente

### Para Hardware Limitado
```bash
docker exec -it ollama-server ollama pull phi
```
- **Modelo**: phi
- **Motivo**: Menor e mais leve

## üîÑ Trocar de Modelo

### Via Vari√°vel de Ambiente

```bash
# Parar aplica√ß√£o
docker-compose down

# Editar .env
LLM_MODEL=mistral

# Baixar novo modelo (se necess√°rio)
docker exec -it ollama-server ollama pull mistral

# Reiniciar
docker-compose up -d
```

### Modelos Dispon√≠veis

Ver todos os modelos dispon√≠veis:
```bash
docker exec -it ollama-server ollama list
```

Buscar mais modelos:
- https://ollama.com/library

## üöÄ Otimiza√ß√£o de Performance

### Com GPU NVIDIA

1. **Instalar NVIDIA Container Toolkit**:
```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

2. **Descomentar linhas GPU no docker-compose.yml**:
```yaml
ollama:
  image: ollama/ollama:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

3. **Reiniciar**:
```bash
docker-compose down
docker-compose up -d
```

### Verificar Uso de GPU

```bash
# Dentro do container
docker exec -it ollama-server nvidia-smi

# Ver logs do Ollama
docker-compose logs ollama | grep -i gpu
```

## üêõ Troubleshooting

### Erro: "Failed to pull model"

```bash
# Verificar conectividade
docker exec -it ollama-server curl -I https://ollama.com

# Verificar espa√ßo em disco
docker exec -it ollama-server df -h
```

### Erro: "Connection refused"

```bash
# Verificar se Ollama est√° rodando
docker-compose ps ollama

# Ver logs
docker-compose logs ollama

# Reiniciar servi√ßo
docker-compose restart ollama
```

### Erro: "Out of Memory"

```bash
# Usar modelo menor
docker exec -it ollama-server ollama pull phi

# Ou aumentar mem√≥ria do Docker
# Docker Desktop -> Settings -> Resources -> Memory
```

### Respostas Lentas

```bash
# 1. Verificar recursos
docker stats ollama-server

# 2. Usar modelo menor
docker exec -it ollama-server ollama pull mistral

# 3. Atualizar .env
LLM_MODEL=mistral
```

## üìä Compara√ß√£o de Velocidade

Tempos m√©dios de resposta (CPU: 8 cores, 16GB RAM):

| Modelo | Tokens/seg | Tempo para 100 palavras |
|--------|------------|-------------------------|
| phi | ~40 | ~5s |
| mistral | ~25 | ~8s |
| llama3 | ~20 | ~10s |
| llama2 | ~18 | ~12s |

Com GPU (NVIDIA RTX 3060):

| Modelo | Tokens/seg | Tempo para 100 palavras |
|--------|------------|-------------------------|
| phi | ~120 | ~2s |
| mistral | ~80 | ~3s |
| llama3 | ~60 | ~4s |
| llama2 | ~55 | ~5s |

## üîê Privacidade e Seguran√ßa

### Dados Locais

‚úÖ Todo processamento ocorre localmente
‚úÖ PDFs nunca saem do seu ambiente
‚úÖ Sem telemetria ou coleta de dados
‚úÖ Ideal para dados sens√≠veis ou confidenciais

### Recomenda√ß√µes

- Use Ollama para documentos confidenciais
- Mantenha o Ollama atualizado
- Monitore uso de recursos
- Fa√ßa backup dos modelos baixados

## üìö Recursos Adicionais

- **Site Oficial**: https://ollama.com
- **Documenta√ß√£o**: https://github.com/ollama/ollama
- **Modelos**: https://ollama.com/library
- **Discord**: https://discord.gg/ollama

## üÜò Suporte

Problemas com Ollama?
1. Verifique os logs: `docker-compose logs ollama`
2. Consulte a documenta√ß√£o oficial
3. Abra uma issue no GitHub do projeto

---

**Desenvolvido para uso local, privado e gratuito! ü¶ô**
